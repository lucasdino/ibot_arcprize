{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdbf3557-edfb-43c4-a37a-4360c07f9818",
   "metadata": {},
   "source": [
    "## Create Our Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "635cd055-6885-468c-b186-b2ce7a680576",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader of length 6581 successfully created from C:\\Users\\lucas\\Desktop\\Lucas\\Coding\\ML Research\\ARC Prize\\evaluation\\ibot_traindata\\ibot_traindata_aggregate.parquet\n",
      "tensor([[0., 0., 0.],\n",
      "        [5., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Local\\Temp\\ipykernel_23392\\1708929901.py:41: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\b\\abs_8f7uhuge1i\\croot\\pytorch-select_1717607507421\\work\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  return torch.tensor(sample.tolist(), dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, Normalize\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ============== Location for our dataset ================\n",
    "try:\n",
    "    current_file_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    # Fallback for environments where __file__ is not defined (e.g., Jupyter notebooks)\n",
    "    current_file_dir = os.getcwd()\n",
    "\n",
    "root = os.path.abspath(os.path.join(current_file_dir, '..', '..'))\n",
    "PARQUET_FILE = os.path.join(root, 'evaluation', 'ibot_traindata', 'ibot_traindata_aggregate.parquet')\n",
    "\n",
    "\n",
    "# ============== Naive dataset / dataloader implementation ================\n",
    "class ParquetDataset(Dataset):\n",
    "    def __init__(self, parquet_file):\n",
    "        df = pd.read_parquet(parquet_file)\n",
    "        self.samples = df['data'].tolist()\n",
    "        self.tensors = [self.convert_to_tensor(sample) for sample in self.samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensors[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_tensor(sample):\n",
    "        if isinstance(sample, list):\n",
    "            sample = [np.array(sublist, dtype=np.float32) if isinstance(sublist, np.ndarray) else sublist for sublist in sample]\n",
    "            return torch.tensor(sample, dtype=torch.float32)\n",
    "        elif isinstance(sample, np.ndarray):\n",
    "            return torch.tensor(sample.tolist(), dtype=torch.float32)\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported sample type: {type(sample)}\")\n",
    "\n",
    "\n",
    "def get_dataloader(batch_size=1, shuffle=True):\n",
    "    \"\"\"\n",
    "        Returns a dataloader object (loads in data from the global var in this file (PARQUET_FILE).\n",
    "\n",
    "        All elements in the dataloader object are of type 'tensor' (specifically, 3-D tensor but each has only 2-Ds).\n",
    "    \"\"\"\n",
    "    dataset = ParquetDataset(PARQUET_FILE)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    print(f\"Dataloader of length {len(dataloader)} successfully created from {PARQUET_FILE}\")\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "\n",
    "# Usage\n",
    "dataloader = get_dataloader()\n",
    "test_data = None\n",
    "for data in dataloader:\n",
    "    test_data = data[0]\n",
    "    print(test_data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2010dc66-05f0-47cc-89ba-63e6b67fac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAugmentationiBOT(object):\n",
    "    \"\"\"\n",
    "        Updated from original iBOT implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, global_crops_number, local_crops_number, pad_to_32=True):\n",
    "        self.global_crops_number = global_crops_number\n",
    "        self.local_crops_number = local_crops_number\n",
    "        self.pad_to_32 = pad_to_32\n",
    "\n",
    "    def pad_image(self, image):\n",
    "        \"\"\" \n",
    "            Returns padded image, height, and width.\n",
    "        \n",
    "            Padded image is either 32x32 or the largest multiple of 4 greater than the height and width of the input image,\n",
    "            with padding set to -1, and image is placed in the top left corner.\n",
    "        \"\"\"\n",
    "        if isinstance(image, np.ndarray):\n",
    "            height, width = image.shape\n",
    "        elif isinstance(image, torch.Tensor):\n",
    "            height, width = image.size()\n",
    "            image = image.numpy()\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a 2-D NumPy array or a PyTorch tensor.\")\n",
    "\n",
    "        if self.pad_to_32:\n",
    "            padded_height = 32\n",
    "            padded_width = 32\n",
    "        else:\n",
    "            padded_height = (height + 3) // 4 * 4\n",
    "            padded_width = (width + 3) // 4 * 4\n",
    "\n",
    "        padded_image = np.full((padded_height, padded_width), -1, dtype=int)\n",
    "        padded_image[:height, :width] = image\n",
    "        return padded_image, height, width\n",
    "\n",
    "    def globalcrop1(self, image):\n",
    "        padded_image, height, width = self.pad_image(image)\n",
    "        padded_height, padded_width = padded_image.shape\n",
    "        max_shift_y = padded_height - height\n",
    "        max_shift_x = padded_width - width\n",
    "        shift_y = random.randint(0, max_shift_y)\n",
    "        shift_x = random.randint(0, max_shift_x)\n",
    "        transformed_image = np.full((padded_height, padded_width), -1, dtype=int)\n",
    "        transformed_image[shift_y:shift_y + height, shift_x:shift_x + width] = padded_image[:height, :width]\n",
    "        return transformed_image\n",
    "\n",
    "    def globalcrop2(self, image):\n",
    "        padded_image, height, width = self.pad_image(image)\n",
    "        digits = list(range(10))\n",
    "        random.shuffle(digits)\n",
    "        mapping = {i: digits[i] for i in range(10)}\n",
    "        transformed_image = np.array([[mapping[pixel] if pixel != -1 else -1 for pixel in row] for row in padded_image])\n",
    "        return transformed_image\n",
    "\n",
    "    def localcrop(self, image):\n",
    "        \"\"\" Placeholder for localcrop, may implement in future. \"\"\"\n",
    "        return image\n",
    "\n",
    "    def __call__(self, image):\n",
    "        \"\"\" Image must be a 2-D NumPy array or a PyTorch tensor \"\"\"\n",
    "        if not (isinstance(image, np.ndarray) or isinstance(image, torch.Tensor)):\n",
    "            raise TypeError(\"Input must be a 2-D NumPy array or a PyTorch tensor.\")\n",
    "    \n",
    "        crops = []\n",
    "        for _ in range(self.global_crops_number):\n",
    "            crop = self.globalcrop1(image)\n",
    "            crops.append(torch.tensor(self.globalcrop2(crop), dtype=torch.float32))\n",
    "        for _ in range(self.local_crops_number):\n",
    "            pass  # crops.append(self.localcrop(image))\n",
    "        return crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c4b246-04e5-4fcf-b2f2-545228633779",
   "metadata": {},
   "outputs": [],
   "source": [
    "class iBOT_Dataloader():\n",
    "    \"\"\" Dataloader wrapper that applies data augmentations / transformations while also loading in our data. \"\"\"\n",
    "    def __init__(self, dataloader, args_dict):\n",
    "        self.dataloader = dataloader\n",
    "        self.data_iterator = iter(dataloader)\n",
    "        self.args = args_dict\n",
    "        self.augmenter = DataAugmentationiBOT(2, 0)\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\" Function that returns the next data point, augmented (see DataAugmentationiBOT), and converted to channels. \"\"\"\n",
    "        generating = True\n",
    "        datum = None\n",
    "        \n",
    "        while generating:\n",
    "            try:\n",
    "                datum = next(self.data_iterator)[0]\n",
    "                generating = False\n",
    "            except StopIteration:\n",
    "                self.data_iterator = iter(self.dataloader)\n",
    "\n",
    "        aug_data = self.augmenter(datum)\n",
    "        aug_data = [self._create_channels(d) for d in aug_data]\n",
    "        return aug_data\n",
    "\n",
    "    def _generate_mask(self, image):\n",
    "        \"\"\" Placeholder for mask generation function. \"\"\"\n",
    "        return (image % 2 == 0).int()  # Example: mask even values\n",
    "\n",
    "    def _create_channels(self, image):\n",
    "        \"\"\"\n",
    "            Accepts list of tensors and returns original image and the masked image.\n",
    "            Returns image with 6 channels:\n",
    "                1) Mask Channel: 1 if pixel is masked, 0 otherwise\n",
    "                2) Padding Channel: 1 if pixel is from image, 0 if pixel is padding\n",
    "                3) Value Channels (4x): Binary encoding of pixel value (1-10)\n",
    "        \"\"\"\n",
    "        H, W = image.shape if isinstance(image, np.ndarray) else image.size()\n",
    "        image = image + 1  # Adjust values from 0-9 to 1-10\n",
    "    \n",
    "        mask = self._generate_mask(image)\n",
    "        \n",
    "        padded_image = np.full((H, W), -1, dtype=int)\n",
    "        padded_image[:H, :W] = image\n",
    "        padded_image[mask == 1] = -2  # Apply mask\n",
    "        \n",
    "        mask_channel = mask.numpy() if isinstance(mask, torch.Tensor) else mask\n",
    "        padding_channel = np.ones((H, W), dtype=int)\n",
    "    \n",
    "        num_value_channels = 4\n",
    "        value_channels = np.unpackbits(np.uint8(padded_image.clip(0, 15))[:, :, np.newaxis], axis=2)[:, :, -num_value_channels:]\n",
    "        \n",
    "        original_image_tensor = np.stack([mask_channel, padding_channel] + [value_channels[:, :, i] for i in range(num_value_channels)], axis=0)\n",
    "        \n",
    "        padded_image[mask == 1] = 0\n",
    "        masked_value_channels = np.unpackbits(np.uint8(padded_image.clip(0, 15))[:, :, np.newaxis], axis=2)[:, :, -num_value_channels:]\n",
    "        masked_image_tensor = np.stack([mask_channel, padding_channel] + [masked_value_channels[:, :, i] for i in range(num_value_channels)], axis=0)\n",
    "    \n",
    "        return (torch.tensor(original_image_tensor, dtype=torch.float32), torch.tensor(masked_image_tensor, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47a910f2-343f-4e21-9eec-9b79bb1efeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tensor_list(tensor_list):\n",
    "    cmap = ListedColormap([\n",
    "        '#000000', '#0074D9', '#FF4136', '#2ECC40', '#FFDC00',\n",
    "        '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'\n",
    "    ])\n",
    "    norm = Normalize(vmin=0, vmax=9)\n",
    "    \n",
    "    def plot_with_gray_stripes(ax, tensor):\n",
    "        array = tensor.numpy() if isinstance(tensor, torch.Tensor) else tensor\n",
    "        mask = array == -1\n",
    "        ax.imshow(array, cmap=cmap, norm=norm)\n",
    "        \n",
    "        h, w = array.shape\n",
    "        stripe_pattern = np.ones((h, w, 4))\n",
    "        stripe_pattern[:, :, :3] = 0.6  # Light gray color\n",
    "        stripe_pattern[:, :, 3] = 0.1  # Alpha channel for transparency\n",
    "        \n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                if mask[i, j] and (i + j) % 2 == 0:\n",
    "                    stripe_pattern[i, j, 3] = 0.8  # Make the pattern opaque\n",
    "        \n",
    "        ax.imshow(stripe_pattern, interpolation='none')\n",
    "        ax.axis('off')\n",
    "\n",
    "    for i, tensor in enumerate(tensor_list):\n",
    "        fig, ax = plt.subplots()\n",
    "        plot_with_gray_stripes(ax, tensor)\n",
    "        plt.title(f'Tensor {i+1}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a90b30e-ab27-4a0d-89aa-576f3c553de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGZCAYAAABmNy2oAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAKfElEQVR4nO3bX2jV9R/H8fem5nSJbbYsF/2hKIkUi1AbkSuoi5hpGSRJaQTZXVBSQtGSBMMurC66iOhm5LpIiBHkTW3SRUERCEFFV4OwpCJFNjGa39/Fj98LDtYvzWzr93s84Fx8P/ucD5/vLs5zn3PO2pqmaQoAqqp9ujcAwMwhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKTKu2trbTeoyNjU33Vv8yzz77bA0MDFRvb2+1tbXVli1bpntLELOnewP8f/v4449brl944YUaHR2tDz/8sGX8uuuu+zu3dU7t2bOnli9fXnfffXe9+eab070daCEKTKvVq1e3XPf09FR7e/sp4/8kk5OTNX/+/N/9+bFjx6q9/d+H9KGhob9rW3BavH3EjPfLL7/Uzp07a+nSpTV37tzq6emphx9+uH744YeWeVdccUUNDAzU/v3768Ybb6x58+bV0qVLT/lrfHJysrZt21ZXXnlldXR0VHd3d9100001PDzcMm9kZKRuvvnmmj9/fi1YsKDuuOOOU042zz//fLW1tdXnn39e9913X3V1ddVVV131X+/nP0GAmchJgRnt5MmTtW7duvroo4/qqaeeqr6+vhofH6/BwcHq7++vzz77rObNm5f5Bw8erCeffLK2b99eixcvrjfeeKMeeeSRuvrqq+vWW2+tqqonnniihoaGaufOnXXDDTfUxMREffHFF/XTTz9lnb1799amTZvqzjvvrOHh4Tpx4kTt3r27+vv764MPPqhbbrmlZZ/33ntvbdy4sR577LGamJj4e345cC40MINs3ry56ezszPXw8HBTVc2+ffta5n366adNVTWvvfZaxi6//PKmo6OjGR8fz9jx48eb7u7uZuvWrRm7/vrrm/Xr1//uHqamppolS5Y0y5Yta6ampjJ+7Nix5qKLLmr6+voyNjg42FRV89xzz/2p++3s7Gw2b978p54L54JzLDPae++9VxdccEGtXbu2fv311zxWrFhRF1988SnfSlqxYkVddtllue7o6KhrrrmmxsfHM7Zy5cp6//33a/v27TU2NlbHjx9vWePrr7+uQ4cO1YMPPtjyVs/5559fGzZsqE8++aQmJydbnrNhw4a/8K5h+ogCM9rhw4fryJEjdd5559WcOXNaHt9//339+OOPLfMXLVp0yhpz585teeF/9dVX6+mnn6533323brvtturu7q7169fXN998U1WVt5EuueSSU9ZasmRJnTx5sn7++eeW8d+aC/9EPlNgRrvwwgtr0aJFtX///t/8+YIFC854zc7OztqxY0ft2LGjDh8+nFPD2rVr66uvvkpYvvvuu1Oee+jQoWpvb6+urq6W8ba2tjPeB8xEosCMNjAwUG+//XZNTU3VqlWr/vL1Fy9eXFu2bKmDBw/Wyy+/XJOTk3XttddWb29v7d27t7Zt25YX/ImJidq3b1++kQT/i0SBGW3jxo311ltv1V133VWPP/54rVy5subMmVPffvttjY6O1rp16+qee+45ozVXrVpVAwMDtXz58urq6qovv/yyhoaGWl7sd+/eXZs2baqBgYHaunVrnThxol566aU6cuRIvfjii2d1TwcOHMjXaaempmp8fLzeeeedqqpas2ZN9fT0nNX6cDZEgRlt1qxZNTIyUq+88koNDQ3Vrl27avbs2XXppZfWmjVratmyZWe85u23314jIyO1Z8+empycrN7e3nrooYfqmWeeyZwHHnigOjs7a9euXXX//ffXrFmzavXq1TU6Olp9fX1ndU+Dg4N14MCBXI+NjeUD89HR0erv7z+r9eFstDVN00z3JgCYGXz7CIAQBQBCFAAIUQAgRAGAEAUA4rT/T2HhwoXnch8AnGNHjx79wzlOCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQs0934uuvv34u98EM8uijj073FoBp4qQAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCANHWNE1zOhMXLlx4rvcCwDl09OjRP5zjpABAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoARFvTNM10bwKAmcFJAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgPgXkFVCFIiWdZAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "augmenter = DataAugmentationiBOT(2, 0)\n",
    "plot_tensor_list([test_data] + augmenter(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cd07030-3e63-4b4e-b207-b49fc2afb69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "iBOT_dataloader = iBOT_Dataloader(dataloader, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8499ebe-aa27-4eab-8487-4f614cd0244c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [1., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [1., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])), (tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]))]\n"
     ]
    }
   ],
   "source": [
    "print(iBOT_dataloader.get_data())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
